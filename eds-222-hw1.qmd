---
title: "EDS 222: Homework 1"
date: "10/14/24"
author: "Naomi Moraes"
execute: 
  eval: true
format:
  html:
    code-fold: show
    code-summary: "Show the code"
    toc: true
editor_options: 
  chunk_output_type: console
  echo: false
warning: false
message: false
---

## Background

*(The case study in this exercise is based on reality, but does not include actual observational data.)*

In this exercise we will look at a case study concerning air quality in South Asia. The World Health Organization estimates that air pollution kills an estimated seven million people per year, due to its effects on the cardiovascular and respiratory systems. Out of the 40 most polluted cities in the world, South Asia is home to 37, and Pakistan was ranked to contain the second most air pollution in the world in 2020 (IQAIR, 2020). In 2019, Lahore, Pakistan was the 12th most polluted city in the world, exposing a population of 11.1 million people to increased mortality and morbidity risks.

In this exercise, you are given two datasets from Lahore, Pakistan and are asked to compare the two different data collection strategies from this city. These data are:

-   Crowd-sourced data from air quality monitors located in people's homes. These data are voluntarily collected by individual households who choose to install a monitor in their home and upload their data for public access.

-   Official government data from monitors installed by government officials at selected locations across Lahore. There have been reports that government officials strategically locate monitors in locations with cleaner air in order to mitigate domestic and international pressure to clean up the air.

::: callout-note
All data for EDS 222 will be stored on the Taylor server, in the shared `/courses/eds-222/data/` directory. Please see material from EDS 214 on how to access and retrieve data from Taylor. These data are small; all compute can be handled locally. Thanks to Bren PhD student Fatiq Nadeem for assembling these data!
:::

In answering the following questions, please consider the lecture content from class on sampling strategies, as well as the material in Chapter 2 of [*Introduction to Modern Statistics*](https://openintro-ims.netlify.app/data-design). Include in your submission your version of this file "`eds-222-hw1.qmd`" and the rendered HTML output, each containing complete answers to all questions *as well as the associated code*. Questions with answers unsupported by the code will be marked incomplete. Showing your work this way will help you develop the habit of creating reproducible code.

## Assessment

```{r}
# Load libraries

library(tidyverse)
library(here)
library(ggplot2)
```

### Question 1

Load the data from each source and label it as `crowdsourced` and `govt` accordingly. For example:

```{r}
crowdsourced <- readRDS(file.path("data", "airpol-PK-crowdsourced.RDS"))
govt <- readRDS(file.path("data", "airpol-PK-govt.RDS"))
```

::: callout-warning
There's an implicit assumption about file organization in the code above. What is it? How can you make the code work?

Student answer: It is assumed that our relevant data is in the "data" folder. We can also make this work using the "here" package - which creates a path relative to the top-level directory.
:::

##### Q and A

1. These dataframes have one row per pollution observation. How many pollution records are in each dataset?

```{r}
dim(crowdsourced)

dim(govt)
```


  - In the crowdsourced dataframe there are: 5488 pollution observation records
  - In the govt dataframe there are: 1960 pollution observation records

2. Each monitor is located at a unique latitude and longitude location. How many unique monitors are in each dataset?

```{r}
long_crowd <- n_distinct(crowdsourced$longitude)
lat_crowd <- n_distinct(crowdsourced$latitude)
```

```{r}
long_crowd
lat_crowd
```

There are 14 unique monitors in the crowdsourced dataset.

```{r}
long_govt <- n_distinct(govt$longitude)
lat_govt <- n_distinct(govt$latitude)
```

```{r}
long_govt
lat_govt
```

There are 4 unique monitors in the government dataset.

::: callout-tip
`group_by(longitude,latitude)` and `cur_group_id()` in `dplyr` will help in creating a unique identifier for each (longitude, latitude) pair.
:::

```{r}

# Create unique identifiers for each dataset

crowdsourced <- crowdsourced %>%
                group_by(longitude,latitude) %>%
                mutate(unique_id = cur_group_id()) 

govt <- govt %>%
        group_by(longitude,latitude) %>%
        mutate(unique_id = cur_group_id()) 
```


### Question 2

The goal of pollution monitoring in Lahore is to measure the average pollution conditions across the city.

##### Q and A

1.  What is the *population* in this setting? Please be precise.

- The population in this setting, is the air-quality across Lahore, Pakistan.

2.  What are the *samples* in this setting? Please be precise.

- The samples are the 1) air quality monitor data volunteered by some households in Lahore, Pakistan; as well as 2) official air quality government data from monitors installed by government officials at selected locations across Lahore.

3.  These samples were not randomly collected from across locations in Lahore. Given the sampling approaches described above, discuss possible biases that may enter when we use these samples to construct estimates of population parameters.
  
  + 3a. In the crowdsourced data, biases present may include the:

    - Hawthorne Effect, as the participants may act differently as they know their air-quality data will be reported;
    - Non-response Bias, as there may be a difference in the output of air-quality data between those who volunteer to publish their data and those who don't

  + 3b. In the government data, biases present may include the:

    - Reporting Bias, as the Lahore government officials may selectively not report data they deem "bad";
    - Insensitive Measure Bias, as the Lahore government officals may have used an insufficiently accurate method of testing, to provide the outcome they desire.

### Question 3

1.  For both the government data and the crowd-sourced data, report the sample mean, sample minimum, and sample maximum value of PM 2.5 (measured in $\mu g/m^3$).

```{r}
# Government Data
# Calculate sample mean, sample minimum, and sample maximum
govt_mean <-mean(govt$PM)
govt_min <- min(govt$PM)
govt_max <- max(govt$PM)

print(paste("The sample mean, sample minimum, and sample maximum of PM 2.5 (μ g/m^3) for the govt data is", govt_mean,",", govt_min,", and", govt_max, "respectively."))

# Crowdsourced Data
# Calculate sample mean, sample minimum, and sample maximum
crowd_mean <-mean(crowdsourced$PM)
crowd_min <- min(crowdsourced$PM)
crowd_max <- max(crowdsourced$PM)

print(paste("The sample mean, sample minimum, and sample maximum of PM 2.5 (μ g/m^3) for the crowdsourced data is", crowd_mean,",", crowd_min,", and", crowd_max, "respectively."))
```

2.  Discuss any key differences that you see between these two samples.

    - Key differences between the statistics for the two samples are that for the crowdsourced data the sample mean, sample minimum, and sample maximum are quite a bit larger. 

3.  Are the differences in mean pollution as expected, given what we know about the sampling strategies?

    - Considering what we know about the two sampling strategies, the aforementioned outcome is not unexpected - as it seems that the statistics from the government data shows an increased likelihood that the government is obfuscating the true air-quality data, through few, geographically selective monitors.

### Question 4

Use the location of the air pollution stations for both of the sampling strategies to generate a map showing locations of each observation. Color the two samples with different colors to highlight how each sample obtains measurements from different parts of the city.

::: callout-tip
`longitude` indicates location in the *x*-direction, while `latitude` indicates location in the *y*-direction. With `ggplot2` this should be nothing fancy. We'll do more spatial data in `R` later in the course.
:::

```{r}
pollution_map <- ggplot(NULL, aes(x = longitude, y = latitude)) +
  geom_point(data = govt,
             color = "red") +
  geom_point(data = crowdsourced,
             color = "blue") +
  xlab("Longitude") +
  ylab("Latitude") +
  ggtitle("Positions of Government and Crowdsourced Air-Quality Monitors")

pollution_map
```

### Question 5

The local newspaper in Pakistan, *Dawn*, claims that the government is misreporting the air pollution levels in Lahore. Do the locations of monitors in question 4, relative to crowd-sourced monitors, suggest anything about a possible political bias?

  - The locations of the four government monitors are in a small clustered spot, whereas the fourteen crowdsourced monitors are far more spread out and more widely and randomly arranged. This suggests a strong possibility that the government is misreporting the air pollution levels in Lahore, as they may be taking data from a relatively clean, and not as polluted area in order to misrepresent their air-quality to the international community. (This may be done in order to stop international pressure to mitigate air pollution practices.)

### Question 6

Given the recent corruption in air quality reporting, the Prime Minister of Pakistan has hired an independent body of environmental data scientists to create an unbiased estimate of the mean PM 2.5 across Lahore using some combination of both government stations and crowd sourced observations.

NASA's satellite data indicates that the average PM across Lahore is 89.2 $\mu g/m^3$. Since this is the most objective estimate of population-level PM 2.5 available, your goal is to match this mean as closely as possible by creating a new ground-level monitoring sample that draws on both the government and crowd-sourced samples.

#### Question 6.1

First, generate a *random sample* of size $n=1000$ air pollution records by (i) pooling observations across the government and the crowd-sourced data; and (ii) drawing observations at random from this pooled sample.

::: callout-tip
`bind_rows()` may be helpful.
:::
```{r}
# Create stacked dataframe of crowdsourced and govt dfs

combined_dataframe <- rbind(crowdsourced, govt)

# Pull 1000 random observations from pooled sample

combined_random_samples <- combined_dataframe[sample(nrow(combined_dataframe), size = 1000, replace = FALSE),]
```

```{r}
print(paste("The sample mean of this pooled and randomly selected data-set is", mean(combined_random_samples$PM)))
```


Second, create a *stratified random sample*. Do so by (i) stratifying your pooled data-set into strata of 0.01 degrees of latitude, and (ii) randomly sampling 200 air pollution observations from each stratum.

```{r}
# Stratify pooled data-set into strata of 0.01 degrees of latitude
# randomly sample 200 observations from each stratum

stratified_samples <- combined_dataframe %>% 
                      mutate(strat_lat = trunc(latitude*10^2)/10^2) %>%
                      group_by(strat_lat) %>%
                      sample_n(size=200, replace = FALSE)
```

```{r}

print(paste("The sample mean of this pooled, stratified, and randomly selected data-set is", mean(stratified_samples$PM)))
```


#### Question 6.2

Compare estimated means of PM 2.5 for each sampling strategy to the NASA estimate of 89.2 $\mu g/m^3$. Which sample seems to match the satellite data best? What would you recommend the Prime Minister do? Does your proposed sampling strategy rely more on government or on crowd-sourced data? Why might that be the case?

The sample that seems to match the NASA satellite data best is the crowdsourced data as the difference between the estimated means of PM 2.5 - between this and the NASA data set - is the smallest when compared to the differences between the estimated means of the NASA data and government data; pooled, stacked and randomly selected data; or the pooled, stratified, and randomly selected data.

If the government monitors can not be moved, I would recommend that the Prime Minister use a stratified and randomly selected data sampling strategy with the crowdsourced data. If the government monitors can be moved, I recommend that they are moved so that they gather data from not-clustered points, and then use a pooled, stratified, and randomly selected data sampling strategy. (This is because it would be good to have non-self reported/ volunteered data - due to the bias impacts described previously. However, not at the cost of accuracy.) 
